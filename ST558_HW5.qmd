---
title: "ST558_HW5"
author: "Charles Lane"
format: html
editor: visual

---

## Task 1 - Conceptual Questions

### Question 1: What is the purpose of cross-validation when fitting a random forest model?
> Selecting a random forest model will result in random predictors used in each bootstrap sample. Cross-validaation is used to determine the best number of predictors to use for the random forest input.

### Question 2: Describe the Bagged Tree Algorithm
> When given a sample set, using the bagged tree algorithm creates subset samples from the original sample called "bootstrap" samples. A model fit is then generated for each bootstrap sample as opposed to the overall sample, and the prediction selected is the average of all bootstrap model predictions generated for the set.

### Question 3: What is meant by a General Linear Model?
> Simply put, a General Linear Model is a linear regression model, which is used for samples with normal distributions and can use simple linear regression, or multiple linear regression.

### Question 4: When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?
> An interaction term accounts for the effect of a predictor on the response in the presence of another particular predictor. For example, cholesterol may be seen to have a certain linear relationship to Heart Disease status, but that linear relationship may have a higher or lower slope based on if the predictors are Male or Female. In that sense, an interaction between Sex & Cholesterol would represent this relationship.

### Question 5: Why do we split our data into a training and test set?
> If using the same data for both test & training, there is a risk of overtraining the underlying data, or fitting the model too perfectly to the explicit data set used to train. A separate test set is helpful in reducing a metric for error when predicting based on any dataset, not just the given set.

## Task 2 - Fitting Models

### EDA / Preparation

> Load the requisite packages. In order to explore and clean the target data set, I will load tidyverse.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(gbm)
```


####Sub-Task 1: Understanding the Data:

> I will start by reading in the 'heart' data from a .csv file downloaded and added to the R repository - "heart.csv". This file uses comma-delimited values, so the read_csv() function from 'readr' package can be used. The spec() function is used to reveal all columns in the dataset and show their data types.

```{r}
heart <- read_csv("heart.csv", col_names = TRUE)
spec(heart)
```

> Now that the data has been read into an object, I will create a usable tibble.

```{r}
heart <- as_tibble(heart)
head(heart)
```

> I would like to find the data columns which include NA values. I can use the is.na() function to determine this. I looped through a base vector called "NA_vec" which was simply constructed with column names matching those of the 'heart' tibble and vector values associated with each column number. Thus there will be the same number of entries in the vector as there are columns in the heart data set. The loop overwrites those base values with the results of a sum of each time is.na is TRUE within that column. The resulting NA_vec shows no NA values.

```{r}
NA_vec <- c(seq_along(heart))
names(NA_vec) <- names(heart)

for (i in seq_along(heart)) {
  NA_vec[i] <- sum(is.na(heart[i]))
}

NA_vec
```

> While there are no NA values in the data set. Looking at the heart data set does show several values of 0 for cholesterol. Given that it is reasonable to assume no individual would have a cholesterol measurement of 0, these values can be considered missing, or functionally NA. I will replace these "0" values with NA to avoid confusion in the future.

> The resulting count of how many Cholesterol measurements are then N/A'ed is calculated specifically for that column.

```{r}
heart$Cholesterol[heart$Cholesterol==0] <- NA

sum(is.na(heart$Cholesterol))
```

> There are 918 total observations for this data set and 172 of them are missing Cholesterol measurements. I have removed these NA'ed observations from the data set for future ease.

> Additionally, the summary() function is used to offer an overview of each variable/column's attributes.

```{r}
heart_2 <- heart |>
  drop_na()

summary(heart_2)
```

####Sub-Task 2: Update Data
> Given that my 'HeartDisease' variable has values of 0 & 1 & is a double() data type, I will update it to be a categorical variable 'Heart_Disease' with values of YES or NO, then remove the original 'HeartDisease' & 'STSlope' variables.

```{r}
heart_3 <- heart_2 |>
  mutate(
    Heart_Disease = ifelse(HeartDisease == 0, "NO",
                           ifelse(HeartDisease == 1,  "YES", NA))) |>
  select(!HeartDisease & !ST_Slope)

heart_3$Heart_Disease <- as.factor(heart_3$Heart_Disease)

head(heart_3)
```

#### Sub-Task 3: Create numeric variables

> For future tasks (using KNN to predict), numeric values are necessary for each predictor. To address my current data set's character variables, I will use the caret package's dummyVars() & predict() functions to create representative numeric variables for several character columns, i.e. Sex, ExerciseAngina, ChestPainType, and RestingECG.

```{r}
dummies <- dummyVars(Heart_Disease ~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_3)
dum_df <- predict(dummies, newdata = heart_3)

head(dum_df)
```
> I'm not sure why the Warning that Heart_Disease is not a factor exists in the preceding output, because it was explicitly made so in preceding actions. A quick test was used to verify:

```{r}
is.factor(heart_3$Heart_Disease)
```

> At this point, I will continue with next steps and determine if there are any other warnings/errors which occur based on this variable's factor status.

> Now to combine the resulting dum_df data set which includes the dummy columns to the original heart_3 data set into a heart_4 data set.

```{r}
heart_4 <- data.frame(heart_3, dum_df)

head(heart_4)
```

### Split the Data

> Prior to fitting a model, we need to split the data into train and test data sets. We can do this with the createDataParticion() function from the caret package. This includes at first setting the seed so that the random number generator used for random number generation/splitting from data sets can be reproducible.

```{r}
#Set the seed so random number generation can be replicated.
set.seed(90)
trainIndex <- createDataPartition(heart_4$Heart_Disease, p = 0.7, list = FALSE)
heartTrain <- heart_4[trainIndex, ]
heartTest <- heart_4[-trainIndex, ]

```

### Fitting using kNN

> The first model used to fit the data will be the kNN model. Using the caret package, I first set a trainControl method. In this case, it is using repeated cross-validation. This means that a cross-validation step is carried out, in our case with 10 subsets, but the CV is also repeated. In our case, we will repeat 3 times and essentially end with an average of the results of the 3 repetitions.

> This training control method is set as an object and entered as an argument for the ultimate goal in the train() function. This train() function is from the caret package and will be used to train the data set to determine an optimal k value.

```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(3333)
knn_fit <- train(Heart_Disease ~., data = heartTrain, method = "knn",
 trControl=trctrl,
 preProcess = c("center", "scale"),
 tuneGrid = data.frame(k = c(1:40)),
 tuneLength = 10)
```

> Given the computational requirements of the above training, I'll setup a separate code chunk to show the knn_fit result:

```{r}
knn_fit
```
> The above result can now be tested using prediction and assessed for accuracy using the confusionMatrix() function. The first step is to actually generate a predicted observation dataframe based on our model fit.

```{r}
test_pred <- predict(knn_fit, newdata = heartTest)
test_pred
```

> I'm not able to innately assess accuracy from that output, so here's the confusionMatrix function to help assess:

```{r}
confusionMatrix(test_pred, heartTest$Heart_Disease)
```

### Logistic Regression
> Eventually, I would like to be able to compare several different prediction models for supervised learning. Given that the data set in question is attempting to predict a factor outcome of whether an individual has heart disease or not. The next step in this process is to use the previously processed data set and fit several different logistic regression models to it.

> For this analysis, variables can be characters (i.e. dummy variables are not necessary), so I will start with the "heart_3" data set, as opposed to the "heart_4" set used for the kNN investigation.

> The three models I will use are: /
> 1) Age
> 2) Age and Cholesterol, with interaction effects
> 3) Age, Cholesterol, and Sex, with interaction effects

> Before beginning, I'd like to construct a plot with Age vs the proportion of observations having Heart Disease. While Age feels like a reasonable assumption to show a correlation to Heart Disease status, I'd like to visualize it first. This plot requires using 'heart_2', which was pre-factorization of Heart Disease variable, because the proportion requires doubles to be calculated.

```{r}
distSum <- heart_2 |>
  group_by(Age) |>
  summarize(propDisease = mean(HeartDisease), n = n())

ggplot(distSum, aes(x=Age, y=propDisease, size = n)) +
  geom_point(stat = "identity", aes(size = n))
```

> It does seem like there's somewhat of a linear relationship, but now I'll start with the actual model development, starting with simply Disease status based on Age. These models will use the heart_3 data set, constructed with Heart Disease as factor variable.

```{r}
#Create a train & test data set from the heart_3 set
#Set the seed so random number generation can be replicated.
set.seed(90)
train2Index <- createDataPartition(heart_3$Heart_Disease, p = 0.7, list = FALSE)
heartTrain2 <- heart_3[trainIndex, ]
heartTest2 <- heart_3[-trainIndex, ]

```

#### Model 1:

```{r}
#glm fit based on Age as the only predictor
(glmFit1 <- train(Heart_Disease ~ Age,
                 data = heartTrain2,
                 method = "glm",
                 trControl=trctrl,
                 family = "binomial"))
```

> summary

```{r}
summary(glmFit1)
```

#### Model 2:

```{r}
#glm fit based on Age & Cholesterol as predictors, as well as their
#interaction effects.
(glmFit2 <- train(Heart_Disease ~ Age*Cholesterol,
                 data = heartTrain,
                 method = "glm",
                 trControl=trctrl,
                 family = "binomial"))
```

> Summary

```{r}
summary(glmFit2)
```

#### Model 3:

```{r}
#glm fit based on Age, Cholesterol, and Sex as predictors
(glmFit3 <- train(Heart_Disease ~ Age + Cholesterol + Sex,
                 data = heartTrain,
                 method = "glm",
                 trControl=trctrl,
                 family = "binomial"))

```

> Summary

```{r}
summary(glmFit3)
```
> The models have AIC values:
> Model 1 AIC = 686.25
> Model 2 AIC = 671.25
> Model 3 AIC = 616.53

> Of the models selected, the 3rd model has the lowest AIC and the best fit

#### Predicting and Analyzing model results

> I'll use the method used in the "Fitting using kNN" section above to predict responses based on the logistic regression models above. I'll also use the method of cross-validation used to get an "average" response outcome.

```{r}
(confusionMatrix(glmFit3, newdata = heartTest2))
```

### Tree Models

> Similar to the above sequence, I will experiment with various tree models and determine which one of the three is the best model/fit.

> Model 1: Sex & Chest Pain Type
> Model 2: Resting BP & Resting ECG
> Model 3: Age, Sex, Resting BP, and interaction effects

#### Tree Model 1:

```{r}
(treeFit1 <- train(Heart_Disease ~ Sex*ChestPainType,
                  data = heartTrain2,
                  method = "rpart",
                  trControl=trctrl,
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cp = seq(0, 0.1, by = 0.001)),
                  tuneLength = 10))
```

#### Tree Model 2:

```{r, warning=FALSE, message=FALSE}
(treeFit2 <- train(Heart_Disease ~ RestingBP + RestingECG,
                  data = heartTrain2,
                  method = "rf",
                  trControl=trctrl,
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(mtry = c(1:10)),
                  tuneLength = 10))
```

#### Tree Model 3:

```{r}
(treeFit3 <- train(Heart_Disease ~ Age*Sex*RestingBP,
                  data = heartTrain2,
                  method = "gbm",
                  trControl=trctrl,
                  preProcess = c("center", "scale"),
                  tuneGrid = expand.grid(n.trees = c(25, 50, 100, 200),
                                        interaction.depth = c(1, 2, 3),
                                        shrinkage = c(0.1),
                                        n.minobsinnode = c(10)),
                  tuneLength = 10,
                  verbose = FALSE))
```

#### Predicting and Analyzing Model Results

> We will use the confusionMatrix() function to asess accuray for each of the 3 Tree Models fitted above.

```{r}
#For Tree Model 1: Classification Tree Model
(confusionMatrix(treeFit1, newdata = heartTest2))
```

```{r}
#For Tree Model 2: Random Forest
(confusionMatrix(treeFit2, newdata = heartTest2))
```

```{r}
#For Tree Model 3: Boosted Tree
(confusionMatrix(treeFit3, newdata=heartTest2))
```

> The results from the above assessments give the following:

> Classification Tree (Sex, ChestPainType, and interaction effects)
> Accuracy: 0.761

> Random Forest (Resting BP & Resting ECG)
> Accuracy: 0.5551

> Boosted Tree (Age, Sex, Resting BP, and interaction effects)
> Accuracy: 0.6801

### Wrap Up

> Given the accuracy measurements of all preceding models, the model with the highest accuracy when acting on the test set is the Classification Tree model with Sex, ChestPainType, and their interaction effects. With a model accuracy of 0.761.