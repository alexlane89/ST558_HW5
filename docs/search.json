[
  {
    "objectID": "ST558_HW5.html",
    "href": "ST558_HW5.html",
    "title": "ST558_HW5",
    "section": "",
    "text": "Selecting a random forest model will result in random predictors used in each bootstrap sample. Cross-validaation is used to determine the best number of predictors to use for the random forest input.\n\n\n\n\n\nWhen given a sample set, using the bagged tree algorithm creates subset samples from the original sample called “bootstrap” samples. A model fit is then generated for each bootstrap sample as opposed to the overall sample, and the prediction selected is the average of all bootstrap model predictions generated for the set.\n\n\n\n\n\nSimply put, a General Linear Model is a linear regression model, which is used for samples with normal distributions and can use simple linear regression, or multiple linear regression.\n\n\n\n\n\nAn interaction term accounts for the effect of a predictor on the response in the presence of another particular predictor. For example, cholesterol may be seen to have a certain linear relationship to Heart Disease status, but that linear relationship may have a higher or lower slope based on if the predictors are Male or Female. In that sense, an interaction between Sex & Cholesterol would represent this relationship.\n\n\n\n\n\nIf using the same data for both test & training, there is a risk of overtraining the underlying data, or fitting the model too perfectly to the explicit data set used to train. A separate test set is helpful in reducing a metric for error when predicting based on any dataset, not just the given set."
  },
  {
    "objectID": "ST558_HW5.html#task-1---conceptual-questions",
    "href": "ST558_HW5.html#task-1---conceptual-questions",
    "title": "ST558_HW5",
    "section": "",
    "text": "Selecting a random forest model will result in random predictors used in each bootstrap sample. Cross-validaation is used to determine the best number of predictors to use for the random forest input.\n\n\n\n\n\nWhen given a sample set, using the bagged tree algorithm creates subset samples from the original sample called “bootstrap” samples. A model fit is then generated for each bootstrap sample as opposed to the overall sample, and the prediction selected is the average of all bootstrap model predictions generated for the set.\n\n\n\n\n\nSimply put, a General Linear Model is a linear regression model, which is used for samples with normal distributions and can use simple linear regression, or multiple linear regression.\n\n\n\n\n\nAn interaction term accounts for the effect of a predictor on the response in the presence of another particular predictor. For example, cholesterol may be seen to have a certain linear relationship to Heart Disease status, but that linear relationship may have a higher or lower slope based on if the predictors are Male or Female. In that sense, an interaction between Sex & Cholesterol would represent this relationship.\n\n\n\n\n\nIf using the same data for both test & training, there is a risk of overtraining the underlying data, or fitting the model too perfectly to the explicit data set used to train. A separate test set is helpful in reducing a metric for error when predicting based on any dataset, not just the given set."
  },
  {
    "objectID": "ST558_HW5.html#task-2---fitting-models",
    "href": "ST558_HW5.html#task-2---fitting-models",
    "title": "ST558_HW5",
    "section": "Task 2 - Fitting Models",
    "text": "Task 2 - Fitting Models\n\nEDA / Preparation\n\nLoad the requisite packages. In order to explore and clean the target data set, I will load tidyverse.\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(gbm)\n\n####Sub-Task 1: Understanding the Data:\n\nI will start by reading in the ‘heart’ data from a .csv file downloaded and added to the R repository - “heart.csv”. This file uses comma-delimited values, so the read_csv() function from ‘readr’ package can be used. The spec() function is used to reveal all columns in the dataset and show their data types.\n\n\nheart &lt;- read_csv(\"heart.csv\", col_names = TRUE)\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspec(heart)\n\ncols(\n  Age = col_double(),\n  Sex = col_character(),\n  ChestPainType = col_character(),\n  RestingBP = col_double(),\n  Cholesterol = col_double(),\n  FastingBS = col_double(),\n  RestingECG = col_character(),\n  MaxHR = col_double(),\n  ExerciseAngina = col_character(),\n  Oldpeak = col_double(),\n  ST_Slope = col_character(),\n  HeartDisease = col_double()\n)\n\n\n\nNow that the data has been read into an object, I will create a usable tibble.\n\n\nheart &lt;- as_tibble(heart)\nhead(heart)\n\n# A tibble: 6 × 12\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\n\nI would like to find the data columns which include NA values. I can use the is.na() function to determine this. I looped through a base vector called “NA_vec” which was simply constructed with column names matching those of the ‘heart’ tibble and vector values associated with each column number. Thus there will be the same number of entries in the vector as there are columns in the heart data set. The loop overwrites those base values with the results of a sum of each time is.na is TRUE within that column. The resulting NA_vec shows no NA values.\n\n\nNA_vec &lt;- c(seq_along(heart))\nnames(NA_vec) &lt;- names(heart)\n\nfor (i in seq_along(heart)) {\n  NA_vec[i] &lt;- sum(is.na(heart[i]))\n}\n\nNA_vec\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n\n\nWhile there are no NA values in the data set. Looking at the heart data set does show several values of 0 for cholesterol. Given that it is reasonable to assume no individual would have a cholesterol measurement of 0, these values can be considered missing, or functionally NA. I will replace these “0” values with NA to avoid confusion in the future.\n\n\nThe resulting count of how many Cholesterol measurements are then N/A’ed is calculated specifically for that column.\n\n\nheart$Cholesterol[heart$Cholesterol==0] &lt;- NA\n\nsum(is.na(heart$Cholesterol))\n\n[1] 172\n\n\n\nThere are 918 total observations for this data set and 172 of them are missing Cholesterol measurements. I have removed these NA’ed observations from the data set for future ease.\n\n\nAdditionally, the summary() function is used to offer an overview of each variable/column’s attributes.\n\n\nheart_2 &lt;- heart |&gt;\n  drop_na()\n\nsummary(heart_2)\n\n      Age            Sex            ChestPainType        RestingBP  \n Min.   :28.00   Length:746         Length:746         Min.   : 92  \n 1st Qu.:46.00   Class :character   Class :character   1st Qu.:120  \n Median :54.00   Mode  :character   Mode  :character   Median :130  \n Mean   :52.88                                         Mean   :133  \n 3rd Qu.:59.00                                         3rd Qu.:140  \n Max.   :77.00                                         Max.   :200  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   : 85.0   Min.   :0.0000   Length:746         Min.   : 69.0  \n 1st Qu.:207.2   1st Qu.:0.0000   Class :character   1st Qu.:122.0  \n Median :237.0   Median :0.0000   Mode  :character   Median :140.0  \n Mean   :244.6   Mean   :0.1676                      Mean   :140.2  \n 3rd Qu.:275.0   3rd Qu.:0.0000                      3rd Qu.:160.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:746         Min.   :-0.1000   Length:746         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.5000   Mode  :character   Median :0.0000  \n                    Mean   : 0.9016                      Mean   :0.4772  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n\n####Sub-Task 2: Update Data &gt; Given that my ‘HeartDisease’ variable has values of 0 & 1 & is a double() data type, I will update it to be a categorical variable ‘Heart_Disease’ with values of YES or NO, then remove the original ‘HeartDisease’ & ‘STSlope’ variables.\n\nheart_3 &lt;- heart_2 |&gt;\n  mutate(\n    Heart_Disease = ifelse(HeartDisease == 0, \"NO\",\n                           ifelse(HeartDisease == 1,  \"YES\", NA))) |&gt;\n  select(!HeartDisease & !ST_Slope)\n\nheart_3$Heart_Disease &lt;- as.factor(heart_3$Heart_Disease)\n\nhead(heart_3)\n\n# A tibble: 6 × 11\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, Heart_Disease &lt;fct&gt;\n\n\n\nSub-Task 3: Create numeric variables\n\nFor future tasks (using KNN to predict), numeric values are necessary for each predictor. To address my current data set’s character variables, I will use the caret package’s dummyVars() & predict() functions to create representative numeric variables for several character columns, i.e. Sex, ExerciseAngina, ChestPainType, and RestingECG.\n\n\ndummies &lt;- dummyVars(Heart_Disease ~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_3)\ndum_df &lt;- predict(dummies, newdata = heart_3)\n\nWarning in model.frame.default(Terms, newdata, na.action = na.action, xlev =\nobject$lvls): variable 'Heart_Disease' is not a factor\n\nhead(dum_df)\n\n  SexF SexM ExerciseAnginaN ExerciseAnginaY ChestPainTypeASY ChestPainTypeATA\n1    0    1               1               0                0                1\n2    1    0               1               0                0                0\n3    0    1               1               0                0                1\n4    1    0               0               1                1                0\n5    0    1               1               0                0                0\n6    0    1               1               0                0                0\n  ChestPainTypeNAP ChestPainTypeTA RestingECGLVH RestingECGNormal RestingECGST\n1                0               0             0                1            0\n2                1               0             0                1            0\n3                0               0             0                0            1\n4                0               0             0                1            0\n5                1               0             0                1            0\n6                1               0             0                1            0\n\n\n\nI’m not sure why the Warning that Heart_Disease is not a factor exists in the preceding output, because it was explicitly made so in preceding actions. A quick test was used to verify:\n\n\nis.factor(heart_3$Heart_Disease)\n\n[1] TRUE\n\n\n\nAt this point, I will continue with next steps and determine if there are any other warnings/errors which occur based on this variable’s factor status.\n\n\nNow to combine the resulting dum_df data set which includes the dummy columns to the original heart_3 data set into a heart_4 data set.\n\n\nheart_4 &lt;- data.frame(heart_3, dum_df)\n\nhead(heart_4)\n\n  Age Sex ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n1  40   M           ATA       140         289         0     Normal   172\n2  49   F           NAP       160         180         0     Normal   156\n3  37   M           ATA       130         283         0         ST    98\n4  48   F           ASY       138         214         0     Normal   108\n5  54   M           NAP       150         195         0     Normal   122\n6  39   M           NAP       120         339         0     Normal   170\n  ExerciseAngina Oldpeak Heart_Disease SexF SexM ExerciseAnginaN\n1              N     0.0            NO    0    1               1\n2              N     1.0           YES    1    0               1\n3              N     0.0            NO    0    1               1\n4              Y     1.5           YES    1    0               0\n5              N     0.0            NO    0    1               1\n6              N     0.0            NO    0    1               1\n  ExerciseAnginaY ChestPainTypeASY ChestPainTypeATA ChestPainTypeNAP\n1               0                0                1                0\n2               0                0                0                1\n3               0                0                1                0\n4               1                1                0                0\n5               0                0                0                1\n6               0                0                0                1\n  ChestPainTypeTA RestingECGLVH RestingECGNormal RestingECGST\n1               0             0                1            0\n2               0             0                1            0\n3               0             0                0            1\n4               0             0                1            0\n5               0             0                1            0\n6               0             0                1            0\n\n\n\n\n\nSplit the Data\n\nPrior to fitting a model, we need to split the data into train and test data sets. We can do this with the createDataParticion() function from the caret package. This includes at first setting the seed so that the random number generator used for random number generation/splitting from data sets can be reproducible.\n\n\n#Set the seed so random number generation can be replicated.\nset.seed(90)\ntrainIndex &lt;- createDataPartition(heart_4$Heart_Disease, p = 0.7, list = FALSE)\nheartTrain &lt;- heart_4[trainIndex, ]\nheartTest &lt;- heart_4[-trainIndex, ]\n\n\n\nFitting using kNN\n\nThe first model used to fit the data will be the kNN model. Using the caret package, I first set a trainControl method. In this case, it is using repeated cross-validation. This means that a cross-validation step is carried out, in our case with 10 subsets, but the CV is also repeated. In our case, we will repeat 3 times and essentially end with an average of the results of the 3 repetitions.\n\n\nThis training control method is set as an object and entered as an argument for the ultimate goal in the train() function. This train() function is from the caret package and will be used to train the data set to determine an optimal k value.\n\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nset.seed(3333)\nknn_fit &lt;- train(Heart_Disease ~., data = heartTrain, method = \"knn\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n tuneGrid = data.frame(k = c(1:40)),\n tuneLength = 10)\n\n\nGiven the computational requirements of the above training, I’ll setup a separate code chunk to show the knn_fit result:\n\n\nknn_fit\n\nk-Nearest Neighbors \n\n523 samples\n 21 predictor\n  2 classes: 'NO', 'YES' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 470, 471, 470, 471, 471, 471, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7424407  0.4821460\n   2  0.7539308  0.5056215\n   3  0.7896226  0.5769802\n   4  0.7864296  0.5706019\n   5  0.7972424  0.5923821\n   6  0.7921746  0.5824353\n   7  0.7921505  0.5824437\n   8  0.7927431  0.5837131\n   9  0.7934446  0.5847194\n  10  0.8024311  0.6027240\n  11  0.8037010  0.6053197\n  12  0.8043299  0.6065235\n  13  0.8037252  0.6055844\n  14  0.8005201  0.5991073\n  15  0.8017779  0.6017509\n  16  0.7998791  0.5980063\n  17  0.8082003  0.6146862\n  18  0.7999153  0.5981801\n  19  0.8011490  0.6005713\n  20  0.8024190  0.6032962\n  21  0.7998911  0.5982057\n  22  0.7985728  0.5954368\n  23  0.8024190  0.6034734\n  24  0.7992259  0.5971602\n  25  0.8062409  0.6112760\n  26  0.8031084  0.6048683\n  27  0.8056362  0.6100904\n  28  0.8069424  0.6125765\n  29  0.8005685  0.6000558\n  30  0.8063256  0.6116522\n  31  0.8025036  0.6040191\n  32  0.7999395  0.5988407\n  33  0.8005926  0.6005130\n  34  0.8005685  0.6003643\n  35  0.8043662  0.6079941\n  36  0.8094582  0.6182223\n  37  0.8069182  0.6132477\n  38  0.8075714  0.6146415\n  39  0.8062651  0.6119919\n  40  0.8082003  0.6160253\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 36.\n\n\n\nThe above result can now be tested using prediction and assessed for accuracy using the confusionMatrix() function. The first step is to actually generate a predicted observation dataframe based on our model fit.\n\n\ntest_pred &lt;- predict(knn_fit, newdata = heartTest)\ntest_pred\n\n  [1] YES NO  NO  NO  NO  NO  NO  NO  YES NO  YES YES YES YES YES NO  NO  YES\n [19] NO  NO  NO  YES YES NO  YES YES NO  NO  NO  YES NO  NO  YES NO  NO  NO \n [37] NO  NO  YES YES NO  YES YES NO  NO  NO  YES NO  YES NO  YES YES NO  YES\n [55] YES NO  YES NO  NO  NO  NO  NO  YES NO  NO  YES NO  NO  NO  YES NO  NO \n [73] NO  NO  NO  YES YES YES NO  NO  NO  YES YES NO  NO  YES NO  YES NO  NO \n [91] NO  YES YES YES NO  NO  YES YES YES YES YES YES NO  YES YES YES YES YES\n[109] YES NO  YES YES YES YES YES YES YES YES YES NO  NO  YES NO  YES YES YES\n[127] YES YES NO  NO  NO  YES NO  YES NO  YES YES NO  NO  YES YES YES NO  NO \n[145] NO  NO  NO  YES NO  NO  NO  NO  NO  NO  YES NO  YES NO  NO  NO  YES YES\n[163] NO  YES NO  YES NO  NO  NO  YES YES YES NO  NO  YES YES YES NO  YES NO \n[181] YES YES YES NO  YES YES NO  YES NO  YES YES YES YES YES NO  YES YES YES\n[199] NO  NO  NO  NO  YES NO  YES NO  YES NO  YES NO  NO  NO  NO  YES NO  NO \n[217] YES NO  NO  NO  YES YES NO \nLevels: NO YES\n\n\n\nI’m not able to innately assess accuracy from that output, so here’s the confusionMatrix function to help assess:\n\n\nconfusionMatrix(test_pred, heartTest$Heart_Disease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction NO YES\n       NO  95  20\n       YES 22  86\n                                          \n               Accuracy : 0.8117          \n                 95% CI : (0.7541, 0.8608)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6227          \n                                          \n Mcnemar's Test P-Value : 0.8774          \n                                          \n            Sensitivity : 0.8120          \n            Specificity : 0.8113          \n         Pos Pred Value : 0.8261          \n         Neg Pred Value : 0.7963          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4260          \n   Detection Prevalence : 0.5157          \n      Balanced Accuracy : 0.8116          \n                                          \n       'Positive' Class : NO              \n                                          \n\n\n\n\nLogistic Regression\n\nEventually, I would like to be able to compare several different prediction models for supervised learning. Given that the data set in question is attempting to predict a factor outcome of whether an individual has heart disease or not. The next step in this process is to use the previously processed data set and fit several different logistic regression models to it.\n\n\nFor this analysis, variables can be characters (i.e. dummy variables are not necessary), so I will start with the “heart_3” data set, as opposed to the “heart_4” set used for the kNN investigation.\n\n\nThe three models I will use are: / 1) Age 2) Age and Cholesterol, with interaction effects 3) Age, Cholesterol, and Sex, with interaction effects\n\n\nBefore beginning, I’d like to construct a plot with Age vs the proportion of observations having Heart Disease. While Age feels like a reasonable assumption to show a correlation to Heart Disease status, I’d like to visualize it first. This plot requires using ‘heart_2’, which was pre-factorization of Heart Disease variable, because the proportion requires doubles to be calculated.\n\n\ndistSum &lt;- heart_2 |&gt;\n  group_by(Age) |&gt;\n  summarize(propDisease = mean(HeartDisease), n = n())\n\nggplot(distSum, aes(x=Age, y=propDisease, size = n)) +\n  geom_point(stat = \"identity\", aes(size = n))\n\n\n\n\n\n\n\n\n\nIt does seem like there’s somewhat of a linear relationship, but now I’ll start with the actual model development, starting with simply Disease status based on Age. These models will use the heart_3 data set, constructed with Heart Disease as factor variable.\n\n\n#Create a train & test data set from the heart_3 set\n#Set the seed so random number generation can be replicated.\nset.seed(90)\ntrain2Index &lt;- createDataPartition(heart_3$Heart_Disease, p = 0.7, list = FALSE)\nheartTrain2 &lt;- heart_3[trainIndex, ]\nheartTest2 &lt;- heart_3[-trainIndex, ]\n\n\nModel 1:\n\n#glm fit based on Age as the only predictor\n(glmFit1 &lt;- train(Heart_Disease ~ Age,\n                 data = heartTrain2,\n                 method = \"glm\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n523 samples\n  1 predictor\n  2 classes: 'NO', 'YES' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 470, 470, 471, 471, 471, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6329584  0.2630993\n\n\n\nsummary\n\n\nsummary(glmFit1)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.44964    0.55684  -6.195 5.83e-10 ***\nAge          0.06321    0.01030   6.137 8.41e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 682.25  on 521  degrees of freedom\nAIC: 686.25\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nModel 2:\n\n#glm fit based on Age & Cholesterol as predictors, as well as their\n#interaction effects.\n(glmFit2 &lt;- train(Heart_Disease ~ Age*Cholesterol,\n                 data = heartTrain,\n                 method = \"glm\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n523 samples\n  2 predictor\n  2 classes: 'NO', 'YES' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 470, 471, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6279028  0.2562873\n\n\n\nSummary\n\n\nsummary(glmFit2)\n\n\nCall:\nNULL\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.255e+01  2.631e+00  -4.769 1.85e-06 ***\nAge                2.162e-01  4.811e-02   4.494 7.00e-06 ***\nCholesterol        3.638e-02  1.015e-02   3.584 0.000339 ***\n`Age:Cholesterol` -6.102e-04  1.848e-04  -3.302 0.000961 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 663.25  on 519  degrees of freedom\nAIC: 671.25\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nModel 3:\n\n#glm fit based on Age, Cholesterol, and Sex as predictors\n(glmFit3 &lt;- train(Heart_Disease ~ Age + Cholesterol + Sex,\n                 data = heartTrain,\n                 method = \"glm\",\n                 trControl=trctrl,\n                 family = \"binomial\"))\n\nGeneralized Linear Model \n\n523 samples\n  3 predictor\n  2 classes: 'NO', 'YES' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 470, 471, 471, 470, 471, 471, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6895985  0.3799412\n\n\n\nSummary\n\n\nsummary(glmFit3)\n\n\nCall:\nNULL\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.709550   0.826786  -8.115 4.85e-16 ***\nAge          0.067940   0.011153   6.092 1.12e-09 ***\nCholesterol  0.006039   0.001698   3.557 0.000375 ***\nSexM         1.948813   0.264209   7.376 1.63e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 608.53  on 519  degrees of freedom\nAIC: 616.53\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nThe models have AIC values: Model 1 AIC = 686.25 Model 2 AIC = 671.25 Model 3 AIC = 616.53\n\n\nOf the models selected, the 3rd model has the lowest AIC and the best fit\n\n\n\nPredicting and Analyzing model results\n\nI’ll use the method used in the “Fitting using kNN” section above to predict responses based on the logistic regression models above. I’ll also use the method of cross-validation used to get an “average” response outcome.\n\n\n(confusionMatrix(glmFit3, newdata = heartTest2))\n\nCross-Validated (10 fold, repeated 3 times) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction   NO  YES\n       NO  34.9 13.7\n       YES 17.3 34.1\n                            \n Accuracy (average) : 0.6896\n\n\n\n\n\nTree Models\n\nSimilar to the above sequence, I will experiment with various tree models and determine which one of the three is the best model/fit.\n\n\nModel 1: Sex & Chest Pain Type Model 2: Resting BP & Resting ECG Model 3: Age, Sex, Resting BP, and interaction effects\n\n\nTree Model 1:\n\n(treeFit1 &lt;- train(Heart_Disease ~ Sex*ChestPainType,\n                  data = heartTrain2,\n                  method = \"rpart\",\n                  trControl=trctrl,\n                  preProcess = c(\"center\", \"scale\"),\n                  tuneGrid = data.frame(cp = seq(0, 0.1, by = 0.001)),\n                  tuneLength = 10))\n\nCART \n\n523 samples\n  2 predictor\n  2 classes: 'NO', 'YES' \n\nPre-processing: centered (7), scaled (7) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 470, 470, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7484519  0.4944257\n  0.001  0.7484519  0.4944257\n  0.002  0.7484519  0.4944257\n  0.003  0.7484519  0.4944257\n  0.004  0.7484519  0.4944257\n  0.005  0.7516449  0.5010932\n  0.006  0.7516449  0.5010932\n  0.007  0.7516449  0.5010932\n  0.008  0.7516449  0.5010932\n  0.009  0.7554669  0.5089632\n  0.010  0.7554669  0.5089632\n  0.011  0.7554669  0.5089632\n  0.012  0.7554669  0.5089632\n  0.013  0.7554669  0.5089632\n  0.014  0.7611998  0.5208814\n  0.015  0.7611998  0.5208814\n  0.016  0.7611998  0.5208814\n  0.017  0.7611998  0.5208814\n  0.018  0.7611998  0.5208814\n  0.019  0.7611998  0.5208814\n  0.020  0.7611998  0.5208814\n  0.021  0.7611998  0.5208814\n  0.022  0.7611998  0.5208814\n  0.023  0.7611998  0.5208814\n  0.024  0.7611998  0.5208814\n  0.025  0.7611998  0.5208814\n  0.026  0.7611998  0.5208814\n  0.027  0.7611998  0.5208814\n  0.028  0.7611998  0.5208814\n  0.029  0.7611998  0.5208814\n  0.030  0.7611998  0.5208814\n  0.031  0.7611998  0.5208814\n  0.032  0.7611998  0.5208814\n  0.033  0.7611998  0.5208814\n  0.034  0.7611998  0.5208814\n  0.035  0.7611998  0.5208814\n  0.036  0.7573537  0.5134024\n  0.037  0.7573537  0.5134024\n  0.038  0.7573537  0.5134024\n  0.039  0.7573537  0.5134024\n  0.040  0.7573537  0.5134024\n  0.041  0.7573537  0.5134024\n  0.042  0.7573537  0.5134024\n  0.043  0.7573537  0.5134024\n  0.044  0.7573537  0.5134024\n  0.045  0.7446299  0.4889228\n  0.046  0.7446299  0.4889228\n  0.047  0.7446299  0.4889228\n  0.048  0.7446299  0.4889228\n  0.049  0.7267537  0.4543038\n  0.050  0.7267537  0.4543038\n  0.051  0.7267537  0.4543038\n  0.052  0.7267537  0.4543038\n  0.053  0.7267537  0.4543038\n  0.054  0.7229076  0.4469751\n  0.055  0.7229076  0.4469751\n  0.056  0.7229076  0.4469751\n  0.057  0.7229076  0.4469751\n  0.058  0.7197388  0.4414069\n  0.059  0.7197388  0.4414069\n  0.060  0.7197388  0.4414069\n  0.061  0.7197388  0.4414069\n  0.062  0.7197388  0.4414069\n  0.063  0.7197388  0.4422349\n  0.064  0.7197388  0.4422349\n  0.065  0.7197388  0.4422349\n  0.066  0.7197388  0.4422349\n  0.067  0.7210208  0.4450080\n  0.068  0.7210208  0.4450080\n  0.069  0.7210208  0.4450080\n  0.070  0.7210208  0.4450080\n  0.071  0.7210208  0.4450080\n  0.072  0.7210208  0.4450080\n  0.073  0.7210208  0.4450080\n  0.074  0.7210208  0.4450080\n  0.075  0.7210208  0.4450080\n  0.076  0.7229439  0.4490507\n  0.077  0.7229439  0.4490507\n  0.078  0.7229439  0.4490507\n  0.079  0.7229439  0.4490507\n  0.080  0.7229439  0.4490507\n  0.081  0.7229439  0.4490507\n  0.082  0.7229439  0.4490507\n  0.083  0.7229439  0.4490507\n  0.084  0.7229439  0.4490507\n  0.085  0.7229439  0.4490507\n  0.086  0.7229439  0.4490507\n  0.087  0.7229439  0.4490507\n  0.088  0.7229439  0.4490507\n  0.089  0.7229439  0.4490507\n  0.090  0.7229439  0.4490507\n  0.091  0.7229439  0.4490507\n  0.092  0.7229439  0.4490507\n  0.093  0.7229439  0.4490507\n  0.094  0.7210208  0.4453363\n  0.095  0.7210208  0.4453363\n  0.096  0.7210208  0.4453363\n  0.097  0.7210208  0.4453363\n  0.098  0.7210208  0.4453363\n  0.099  0.7210208  0.4453363\n  0.100  0.7210208  0.4453363\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.035.\n\n\n\n\nTree Model 2:\n\n(treeFit2 &lt;- train(Heart_Disease ~ RestingBP + RestingECG,\n                  data = heartTrain2,\n                  method = \"rf\",\n                  trControl=trctrl,\n                  preProcess = c(\"center\", \"scale\"),\n                  tuneGrid = data.frame(mtry = c(1:10)),\n                  tuneLength = 10))\n\nRandom Forest \n\n523 samples\n  2 predictor\n  2 classes: 'NO', 'YES' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 471, 471, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa     \n   1    0.5551040  0.09562277\n   2    0.5506410  0.08710945\n   3    0.5468795  0.08058912\n   4    0.5449927  0.07684816\n   5    0.5417755  0.07027804\n   6    0.5443517  0.07532134\n   7    0.5373125  0.06145278\n   8    0.5424165  0.07137642\n   9    0.5392235  0.06542680\n  10    0.5360305  0.05902229\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 1.\n\n\n\n\nTree Model 3:\n\n(treeFit3 &lt;- train(Heart_Disease ~ Age*Sex*RestingBP,\n                  data = heartTrain2,\n                  method = \"gbm\",\n                  trControl=trctrl,\n                  preProcess = c(\"center\", \"scale\"),\n                  tuneGrid = expand.grid(n.trees = c(25, 50, 100, 200),\n                                        interaction.depth = c(1, 2, 3),\n                                        shrinkage = c(0.1),\n                                        n.minobsinnode = c(10)),\n                  tuneLength = 10,\n                  verbose = FALSE))\n\nStochastic Gradient Boosting \n\n523 samples\n  3 predictor\n  2 classes: 'NO', 'YES' \n\nPre-processing: centered (7), scaled (7) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 470, 471, 471, 470, 471, 471, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.6710450  0.3408158\n  1                   50      0.6710329  0.3414519\n  1                  100      0.6684930  0.3368812\n  1                  200      0.6799952  0.3591444\n  2                   25      0.6729560  0.3460858\n  2                   50      0.6677673  0.3352570\n  2                  100      0.6799347  0.3596900\n  2                  200      0.6709845  0.3411997\n  3                   25      0.6710692  0.3412681\n  3                   50      0.6705128  0.3396435\n  3                  100      0.6671626  0.3334723\n  3                  200      0.6595549  0.3181688\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 200, interaction.depth =\n 1, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\n\n\nPredicting and Analyzing Model Results\n\nWe will use the confusionMatrix() function to asess accuray for each of the 3 Tree Models fitted above.\n\n\n#For Tree Model 1: Classification Tree Model\n(confusionMatrix(treeFit1, newdata = heartTest2))\n\nCross-Validated (10 fold, repeated 3 times) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction   NO  YES\n       NO  41.1 12.8\n       YES 11.1 35.0\n                           \n Accuracy (average) : 0.761\n\n\n\n#For Tree Model 2: Random Forest\n(confusionMatrix(treeFit2, newdata = heartTest2))\n\nCross-Validated (10 fold, repeated 3 times) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction   NO  YES\n       NO  38.2 30.5\n       YES 14.0 17.3\n                            \n Accuracy (average) : 0.5551\n\n\n\n#For Tree Model 3: Boosted Tree\n(confusionMatrix(treeFit3, newdata=heartTest2))\n\nCross-Validated (10 fold, repeated 3 times) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction   NO  YES\n       NO  35.7 15.5\n       YES 16.5 32.3\n                            \n Accuracy (average) : 0.6801\n\n\n\nThe results from the above assessments give the following:\n\n\nClassification Tree (Sex, ChestPainType, and interaction effects) Accuracy: 0.761\n\n\nRandom Forest (Resting BP & Resting ECG) Accuracy: 0.5551\n\n\nBoosted Tree (Age, Sex, Resting BP, and interaction effects) Accuracy: 0.6801\n\n\n\n\nWrap Up\n\nGiven the accuracy measurements of all preceding models, the model with the highest accuracy when acting on the test set is the Classification Tree model with Sex, ChestPainType, and their interaction effects. With a model accuracy of 0.761."
  }
]